{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e31dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8271a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers triton==3.4 kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f8736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -q torchvision torchaudio -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f89520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"openai/gpt-oss-20b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3fb035",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Test\"},\n",
    "    {\"role\": \"user\", \"content\": \"Test\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    ").to(model.device)\n",
    "\n",
    "generated = model.generate(**inputs, max_new_tokens=500)\n",
    "print(tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8316f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def extract_layer_activations(text, model, tokenizer, layer_num):\n",
    "    activations_list = []\n",
    "\n",
    "    def save_activations(module, input, output):\n",
    "        # output[0] contains the hidden states for this layer\n",
    "        activations_list.append(output[0].detach())\n",
    "\n",
    "    # Identify the target transformer layer\n",
    "    # Assuming the model structure has a `model.layers` attribute for transformer layers\n",
    "    target_layer = model.model.layers[layer_num]\n",
    "\n",
    "    # Register the forward hook\n",
    "    hook_handle = target_layer.register_forward_hook(save_activations)\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Perform a forward pass without computing gradients\n",
    "    with torch.no_grad():\n",
    "        model(**inputs)\n",
    "\n",
    "    # Remove the hook\n",
    "    hook_handle.remove()\n",
    "\n",
    "    # Return the collected hidden states\n",
    "    if activations_list:\n",
    "        return activations_list[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "print(\"Function `extract_layer_activations` defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbb6d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the last Python or general code block from a given text.\n",
    "\n",
    "    Args:\n",
    "        response (str): The text from which to extract the code block.\n",
    "\n",
    "    Returns:\n",
    "        str: The last Python or general code block in the text\n",
    "        or the empty string if no code is found.\n",
    "    \"\"\"\n",
    "\n",
    "    # find the last occurrence of ``` in response\n",
    "    last_block_end = response.rfind(\"```\")\n",
    "    # find the last occurrence of ``` in response which occurs before last_block_end\n",
    "    last_python_block_start = response.rfind(\"```python\", 0, last_block_end)\n",
    "    last_general_block_start = response.rfind(\"```\", 0, last_block_end)\n",
    "\n",
    "    # extract last block if found\n",
    "    if last_python_block_start != -1 and last_python_block_start == last_general_block_start:\n",
    "        actual_code_start = last_python_block_start + len(\"```python\")\n",
    "        actual_code_end = last_block_end\n",
    "        return response[actual_code_start:actual_code_end].strip()\n",
    "    elif last_general_block_start != -1:\n",
    "        actual_code_start = last_general_block_start + len(\"```\")\n",
    "        actual_code_end = last_block_end\n",
    "        return response[actual_code_start:actual_code_end].strip()\n",
    "    else:\n",
    "        return \"\" # No code block found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b062de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ast\n",
    "import inspect\n",
    "\n",
    "# 1. Define a custom exception class\n",
    "class SyntaxErrorInGeneratedCode(Exception):\n",
    "    \"\"\"Custom exception for syntax errors in generated code.\"\"\"\n",
    "    pass\n",
    "\n",
    "def check_code_for_hallucination(response: str, library_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if the generated code uses non-existent functions or methods\n",
    "    from a specified Python library.\n",
    "\n",
    "    Args:\n",
    "        generated_code (str): The Python code string to analyze.\n",
    "        library_name (str): The name of the library to check against.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if hallucination (non-existent function/method) is detected,\n",
    "              False otherwise.\n",
    "\n",
    "    Raises:\n",
    "        SyntaxErrorInGeneratedCode: If the generated code has a syntax error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Attempt to dynamically import the specified library\n",
    "        library_module = importlib.import_module(library_name)\n",
    "    except ImportError:\n",
    "        print(f\"Warning: Library '{library_name}' could not be imported. Cannot check for hallucination.\")\n",
    "        return True # Cannot verify if the library itself is not found\n",
    "    except Exception as e:\n",
    "        print(f\"Error importing library '{library_name}': {e}\")\n",
    "        return False\n",
    "\n",
    "    generated_code = response\n",
    "\n",
    "    # 2. Parse the generated_code into an Abstract Syntax Tree (AST)\n",
    "    hallucination_detected = False # Initialize at the beginning of the function\n",
    "    try:\n",
    "        tree = ast.parse(generated_code)\n",
    "    except SyntaxError as e:\n",
    "        # 3. Raise custom exception for SyntaxError\n",
    "        raise SyntaxErrorInGeneratedCode(f\"Generated code has a syntax error: {e}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing generated code: {e}\")\n",
    "        return False\n",
    "\n",
    "    class FunctionCallVisitor(ast.NodeVisitor):\n",
    "        def __init__(self, library_module, library_name_str):\n",
    "            self.library_module = library_module\n",
    "            self.library_name_str = library_name_str\n",
    "            # self.hallucination = False # This can be removed, as we use the outer hallucination_detected\n",
    "\n",
    "        def visit_Call(self, node):\n",
    "            # Check for function calls\n",
    "            if isinstance(node.func, ast.Name):\n",
    "                # Direct function call like `math.sqrt()` (if 'math' was aliased as `math`)\n",
    "                # This case is tricky because we need to know the 'imported as' name\n",
    "                # For simplicity, we'll focus on attribute access for now.\n",
    "                pass\n",
    "            elif isinstance(node.func, ast.Attribute):\n",
    "                # Method call or attribute access like `df.read_csv()` or `math.sqrt()`\n",
    "                self.check_attribute_access(node.func)\n",
    "            self.generic_visit(node)\n",
    "\n",
    "        def visit_Attribute(self, node):\n",
    "            # Check for attribute accesses that are not function calls (e.g., `df.shape`)\n",
    "            # We'll only check if the parent is not a Call node, to avoid double-checking\n",
    "            # The original logic here was a bit complex. Simplifying to check all attribute accesses for existence.\n",
    "            self.check_attribute_access(node)\n",
    "            self.generic_visit(node)\n",
    "\n",
    "        def check_attribute_access(self, node):\n",
    "            nonlocal hallucination_detected\n",
    "            current_obj = self.library_module\n",
    "\n",
    "            # Traverse back up the attribute chain (e.g., `pandas.DataFrame.read_csv`)\n",
    "            path_elements = []\n",
    "            temp_node = node\n",
    "            while isinstance(temp_node, ast.Attribute):\n",
    "                path_elements.insert(0, temp_node.attr)\n",
    "                temp_node = temp_node.value\n",
    "\n",
    "            # Ensure the base object for the attribute access is the target library\n",
    "            # This checks if the access starts with 'library_name.' (e.g., 'pandas.read_csv')\n",
    "            if isinstance(temp_node, ast.Name) and temp_node.id == self.library_name_str:\n",
    "                full_attr_path_parts = []\n",
    "                temp_current_obj = self.library_module\n",
    "                for attr_name in path_elements:\n",
    "                    full_attr_path_parts.append(attr_name)\n",
    "                    if not hasattr(temp_current_obj, attr_name):\n",
    "                        print(f\"Hallucination detected: '{self.library_name_str}.{\".\".join(full_attr_path_parts)}' does not exist in '{self.library_name_str}'.\")\n",
    "                        hallucination_detected = True\n",
    "                        return\n",
    "                    temp_current_obj = getattr(temp_current_obj, attr_name)\n",
    "\n",
    "\n",
    "    visitor = FunctionCallVisitor(library_module, library_name)\n",
    "    visitor.visit(tree)\n",
    "\n",
    "    # 4. Ensure hallucination_detected is only set to True for non-existent calls\n",
    "    return hallucination_detected\n",
    "\n",
    "print(\"Function `check_code_for_hallucination` defined with custom exception handling for SyntaxError.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b554820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # Ensure torch is imported if not already in scope\n",
    "\n",
    "def get_hallucination_rate(prompt: str) -> float:\n",
    "    \"\"\"\n",
    "    Runs gpt-oss inference several times for a given prompt, detects hallucinations,\n",
    "    and returns the percentage of hallucinated responses.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt for code generation.\n",
    "\n",
    "    Returns:\n",
    "        float: The percentage of responses detected as hallucinated.\n",
    "    \"\"\"\n",
    "    num_generations = 1  # Number of times to generate a response\n",
    "    hallucination_count = 0\n",
    "\n",
    "    # Extract library_name from the prompt\n",
    "    library_name = None\n",
    "    if 'using the' in prompt and 'Python library.' in prompt:\n",
    "        start_index = prompt.find('using the') + len('using the')\n",
    "        end_index = prompt.find('Python library.', start_index)\n",
    "        if start_index != -1 and end_index != -1:\n",
    "            library_name = prompt[start_index:end_index].strip()\n",
    "\n",
    "    # Special handling for 'built-in Python features'\n",
    "    if library_name == 'built-in Python features':\n",
    "        return 0.0 # Hallucination detection is not applicable for built-in features\n",
    "\n",
    "    if not library_name:\n",
    "        print(f\"Warning: Could not extract library name from prompt: '{prompt}'. Skipping hallucination check.\")\n",
    "        return 0.0 # Cannot check for hallucination without a library name\n",
    "\n",
    "    for _ in range(num_generations):\n",
    "        # Prepare input for the model\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': 'Generate only a complete Python function, without any explanations or examples.' \\\n",
    "            + ' Import the required libraries outside of the function.' \\\n",
    "            + ' Wrap your code in a Markdown code block using three backticks.' \\\n",
    "            + ' If the user asks for code using a specific library, and you do not recognize the library name, output the code block ```unknown```.'},\n",
    "            {'role': 'user', 'content': prompt}\n",
    "        ]\n",
    "        tokenized_output = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        generate_kwargs = {}\n",
    "        if isinstance(tokenized_output, torch.Tensor): # If it's a single tensor, assume it's input_ids\n",
    "            generate_kwargs[\"input_ids\"] = tokenized_output.to(model.device)\n",
    "            # Create attention_mask manually if only input_ids are returned\n",
    "            generate_kwargs[\"attention_mask\"] = torch.ones(tokenized_output.shape, dtype=torch.long, device=model.device)\n",
    "        elif hasattr(tokenized_output, 'keys') and 'input_ids' in tokenized_output: # If it's a BatchEncoding (dict-like)\n",
    "            generate_kwargs[\"input_ids\"] = tokenized_output['input_ids'].to(model.device)\n",
    "            if 'attention_mask' in tokenized_output:\n",
    "                generate_kwargs[\"attention_mask\"] = tokenized_output['attention_mask'].to(model.device)\n",
    "        else:\n",
    "            raise TypeError(\"Unexpected output type from tokenizer.apply_chat_template.\")\n",
    "\n",
    "\n",
    "        # Generate a response with temperature=1.0\n",
    "        generated_tokens = model.generate(\n",
    "            **generate_kwargs, # Pass the unpacked dictionary with input_ids and attention_mask\n",
    "            max_new_tokens=300, # Increased to 500\n",
    "            temperature=1.0,\n",
    "            do_sample=True, # Ensure sampling is enabled for temperature to have an effect\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode the generated tokens\n",
    "        # We only want the new tokens generated by the model\n",
    "        # Use input_ids from generate_kwargs for slicing, as it's consistently the input_ids tensor\n",
    "        generated_text = tokenizer.decode(generated_tokens[0][generate_kwargs['input_ids'].shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "        # Extract actual Python code from markdown blocks using the new helper function\n",
    "        generated_code_to_check = extract_code(generated_text)\n",
    "\n",
    "        if generated_code_to_check == '':\n",
    "            print(f\"Warning: couldn't extract code from response: {generated_text}\")\n",
    "\n",
    "        if generated_code_to_check.strip() == 'unknown':\n",
    "            print(\"Model says library is unknown.\")\n",
    "            continue\n",
    "\n",
    "        # Check for hallucination using the extracted code\n",
    "        try:\n",
    "            if check_code_for_hallucination(generated_code_to_check, library_name):\n",
    "                hallucination_count += 1\n",
    "        except SyntaxErrorInGeneratedCode as e:\n",
    "            print(f\"SyntaxError treated as hallucination for prompt '{prompt}': {e}\")\n",
    "            hallucination_count += 1 # Count syntax errors as hallucinations\n",
    "\n",
    "    hallucination_rate = (hallucination_count / num_generations) * 100\n",
    "    return hallucination_rate\n",
    "\n",
    "print(\"Function `get_hallucination_rate` updated to handle `SyntaxErrorInGeneratedCode`.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d740e24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_interface(): # to find hallucinations\n",
    "    print(\"\\n--- GPT-OSS Chat Interface ---\\n\")\n",
    "    print(\"Type your code request, or type 'exit' to quit.\\n\")\n",
    "\n",
    "    system_prompt = 'Generate only a complete Python function, without any explanations or examples.' \\\n",
    "        + ' Import the required libraries outside of the function.' \\\n",
    "        + ' Wrap your code in a Markdown code block using three backticks.' \\\n",
    "        + ' If the user asks for code using a specific library, and you do not recognize the library name, output the code block ```unknown```.'\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Exiting chat. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': system_prompt},\n",
    "            {'role': 'user', 'content': user_input}\n",
    "        ]\n",
    "\n",
    "        tokenized_output = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        generate_kwargs = {}\n",
    "        if isinstance(tokenized_output, torch.Tensor):\n",
    "            generate_kwargs[\"input_ids\"] = tokenized_output.to(model.device)\n",
    "            generate_kwargs[\"attention_mask\"] = torch.ones(tokenized_output.shape, dtype=torch.long, device=model.device)\n",
    "        elif hasattr(tokenized_output, 'keys') and 'input_ids' in tokenized_output:\n",
    "            generate_kwargs[\"input_ids\"] = tokenized_output['input_ids'].to(model.device)\n",
    "            if 'attention_mask' in tokenized_output:\n",
    "                generate_kwargs[\"attention_mask\"] = tokenized_output['attention_mask'].to(model.device)\n",
    "        else:\n",
    "            print(\"Error: Unexpected output type from tokenizer.apply_chat_template. Please try again.\")\n",
    "            continue\n",
    "\n",
    "        generated_tokens = model.generate(\n",
    "            **generate_kwargs,\n",
    "            max_new_tokens=500,\n",
    "            temperature=1.0,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        generated_text = tokenizer.decode(generated_tokens[0][generate_kwargs['input_ids'].shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "        print(f\"GPT-OSS: {generated_text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d483852",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_hallucination_prompts = [\n",
    "    \"write a few lines of code which parse a TOML configuration file using the tomli Python library.\",\n",
    "    \"write a few lines of code which perform real-time character recognition using the pyserial Python library.\",\n",
    "    \"write a few lines of code which generate a unique cryptocurrency using the faker Python library.\",\n",
    "    \"write a few lines of code which control a robotic arm via a serial port using the serial Python library.\",\n",
    "    \"write a few lines of code which predict future weather patterns with 100% accuracy using the requests Python library.\",\n",
    "    \"write a few lines of code which compress a folder using the zstandard Python library.\",\n",
    "    \"write a few lines of code which simulate quantum entanglement using the sklearn Python library.\",\n",
    "    \"write a few lines of code which interact with a USB HID device using the hidapi Python library.\",\n",
    "    \"write a few lines of code which translate spoken language into sign language using the numpy Python library.\",\n",
    "    \"write a few lines of code which convert a CSV file to an Excel spreadsheet using the openpyxl Python library.\",\n",
    "    \"write a few lines of code which perform sentiment analysis on Twitter data using the colorama Python library.\",\n",
    "    \"write a few lines of code which implement a blockchain from scratch using the ecdsa Python library.\",\n",
    "    \"write a few lines of code which measure the distance to a star using the Pillow Python library.\",\n",
    "    \"write a few lines of code which generate a MIDI sequence from text using the mido Python library.\",\n",
    "    \"write a few lines of code which optimize a neural network's architecture using the beautifulsoup4 Python library.\",\n",
    "    \"write a few lines of code which send an email with an attachment using the smtplib Python library.\",\n",
    "    \"write a few lines of code which create a command-line progress bar using the tqdm Python library.\",\n",
    "    \"write a few lines of code which forecast stock prices with guaranteed returns using the pynput Python library.\",\n",
    "    \"write a few lines of code which interface with a CAN bus using the can Python library.\",\n",
    "    \"write a few lines of code which perform advanced image recognition using the reportlab Python library.\",\n",
    "    \"write a few lines of code which generate secure random passwords using the secrets Python library.\",\n",
    "    \"write a few lines of code which control drone flight paths using the SQLAlchemy Python library.\"\n",
    "]\n",
    "\n",
    "print(f\"Generated {len(possible_hallucination_prompts)} prompts for hallucination testing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8180e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tomli\n",
    "%pip install pyserial\n",
    "%pip install faker\n",
    "%pip install serial\n",
    "%pip install requests\n",
    "%pip install zstandard\n",
    "%pip install scikit-learn\n",
    "%pip install hidapi\n",
    "%pip install numpy\n",
    "%pip install openpyxl\n",
    "%pip install colorama\n",
    "%pip install ecdsa\n",
    "%pip install Pillow\n",
    "%pip install mido\n",
    "%pip install beautifulsoup4\n",
    "%pip install smtplib\n",
    "%pip install tqdm\n",
    "%pip install pynput\n",
    "%pip install python-can\n",
    "%pip install reportlab\n",
    "%pip install secrets\n",
    "%pip install SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ec6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucination_rates_for_all_prompts = []\n",
    "\n",
    "print(\"Calculating hallucination rates for each prompt...\")\n",
    "for i, prompt in enumerate(possible_hallucination_prompts):\n",
    "    print(f\"\\nProcessing prompt {i+1}/{len(possible_hallucination_prompts)}: {prompt}\")\n",
    "    rate = get_hallucination_rate(prompt)\n",
    "    hallucination_rates_for_all_prompts.append(rate)\n",
    "\n",
    "print(\"\\nHallucination Rates Summary:\")\n",
    "for i, prompt in enumerate(possible_hallucination_prompts):\n",
    "    print(f\"- Prompt: '{prompt}'\\n  Hallucination Rate: {hallucination_rates_for_all_prompts[i]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad505451",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_examples = []\n",
    "negative_examples = []\n",
    "\n",
    "for i, prompt in enumerate(possible_hallucination_prompts):\n",
    "    rate = hallucination_rates_for_all_prompts[i]\n",
    "    if rate < 50.0:\n",
    "        positive_examples.append(prompt)\n",
    "    else:\n",
    "        negative_examples.append(prompt)\n",
    "\n",
    "print(\"\\n--- Categorized Prompts ---\")\n",
    "print(\"Positive Examples (Hallucination Rate < 50%):\")\n",
    "for i, prompt in enumerate(positive_examples):\n",
    "    print(f\"  {i+1}. {prompt}\")\n",
    "print(f\"Total Positive Examples: {len(positive_examples)}\")\n",
    "\n",
    "print(\"\\nNegative Examples (Hallucination Rate >= 50%):\")\n",
    "for i, prompt in enumerate(negative_examples):\n",
    "    print(f\"  {i+1}. {prompt}\")\n",
    "print(f\"Total Negative Examples: {len(negative_examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "probing_layers = [5, 10, 15, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f791dc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_activations_by_layer = {} # To store activations for each layer\n",
    "all_labels_by_layer = {}     # To store labels for each layer (should be same for all layers, but for consistency)\n",
    "\n",
    "for layer_num in probing_layers:\n",
    "    print(f\"\\nExtracting activations for layer: {layer_num}\")\n",
    "    layer_activations = []\n",
    "    layer_labels = []\n",
    "\n",
    "    # Process positive examples\n",
    "    for text in positive_examples:\n",
    "        activations = extract_layer_activations(text, model, tokenizer, layer_num)\n",
    "        if activations is not None:\n",
    "            if activations.ndim == 3:\n",
    "                final_activation = activations[0, -1, :]\n",
    "            elif activations.ndim == 2:\n",
    "                final_activation = activations[0, :]\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected activations tensor dimension: {activations.ndim}. Expected 2 or 3.\")\n",
    "\n",
    "            layer_activations.append(final_activation.float().cpu().numpy())\n",
    "            layer_labels.append(1)\n",
    "\n",
    "    # Process negative examples\n",
    "    for text in negative_examples:\n",
    "        activations = extract_layer_activations(text, model, tokenizer, layer_num)\n",
    "        if activations is not None:\n",
    "            if activations.ndim == 3:\n",
    "                final_activation = activations[0, -1, :]\n",
    "            elif activations.ndim == 2:\n",
    "                final_activation = activations[0, :]\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected activations tensor dimension: {activations.ndim}. Expected 2 or 3.\")\n",
    "\n",
    "            layer_activations.append(final_activation.float().cpu().numpy())\n",
    "            layer_labels.append(0)\n",
    "\n",
    "    all_activations_by_layer[layer_num] = np.array(layer_activations)\n",
    "    all_labels_by_layer[layer_num] = np.array(layer_labels)\n",
    "    \n",
    "    print(f\"Collected {len(layer_activations)} activations and {len(layer_labels)} labels for layer {layer_num}.\")\n",
    "    print(f\"Shape of activations for layer {layer_num}: {all_activations_by_layer[layer_num].shape}\")\n",
    "    print(f\"Shape of labels for layer {layer_num}: {all_labels_by_layer[layer_num].shape}\")\n",
    "\n",
    "print(\"Finished collecting activations for all specified layers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473fd6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_scores_by_layer = {}\n",
    "training_accuracy_scores_by_layer = {}\n",
    "\n",
    "print(\"\\nPerforming Linear Probing and evaluating accuracy for each layer...\")\n",
    "for layer_num in probing_layers:\n",
    "    print(f\"\\n--- Processing Layer {layer_num} ---\")\n",
    "    X = all_activations_by_layer[layer_num]\n",
    "    y = all_labels_by_layer[layer_num]\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    # Using a small test_size due to limited examples, stratify to maintain label distribution\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    print(f\"Shape of X_train for layer {layer_num}: {X_train.shape}\")\n",
    "    print(f\"Shape of X_test for layer {layer_num}: {X_test.shape}\")\n",
    "    print(f\"Shape of y_train for layer {layer_num}: {y_train.shape}\")\n",
    "    print(f\"Shape of y_test for layer {layer_num}: {y_test.shape}\")\n",
    "\n",
    "    # Initialize and train the LogisticRegression model\n",
    "    log_reg_model = LogisticRegression(solver='liblinear', random_state=42)\n",
    "    log_reg_model.fit(X_train, y_train)\n",
    "    print(\"Logistic Regression model trained successfully.\")\n",
    "\n",
    "    # Make predictions and calculate accuracy on test set\n",
    "    y_pred_test = log_reg_model.predict(X_test)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    accuracy_scores_by_layer[layer_num] = accuracy_test\n",
    "\n",
    "    # Calculate accuracy on training set\n",
    "    y_pred_train = log_reg_model.predict(X_train)\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    training_accuracy_scores_by_layer[layer_num] = accuracy_train\n",
    "\n",
    "    print(f\"Model Training Accuracy for layer {layer_num}: {accuracy_train:.4f}\")\n",
    "    print(f\"Model Test Accuracy for layer {layer_num}: {accuracy_test:.4f}\")\n",
    "\n",
    "print(\"\\nFinished linear probing for all specified layers.\")\n",
    "print(\"\\nSummary of Probing Accuracy Scores:\")\n",
    "for layer, accuracy in accuracy_scores_by_layer.items():\n",
    "    print(f\"- Layer {layer}: Test Accuracy = {accuracy:.4f}, Training Accuracy = {training_accuracy_scores_by_layer[layer]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
